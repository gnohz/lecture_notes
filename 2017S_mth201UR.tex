\input{notes_preambles.tex}

\begin{document}
	% \let\ref\Cref
	\title{\bf{MTH 201: Introduction to Probability}}
	\date{Spring 2017, University of Rochester}
	\author{Jie Zhong}

	\blfootnote{I'd love to hear your feedback. Feel free to email me at \href{mailto:jiezhongmath@gmail.com}{jiezhongmath@gmail.com}.}

	\blfootnote{% See \href{http://cthomson.ca/notes}{cthomson.ca/notes} for updates.
		% \ifdefined\sha % Also, \commitDateTime should be defined.
		Last modified: \today 
    % \commitDateTime{} ({\href{https://github.com/christhomson/lecture-notes/commit/\sha}{\sha}}).
		% \fi
	}

	\maketitle
	\newpage
	\tableofcontents
	\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction and sets} \lecture{January 18, 2017}
  \subsection{Course structure}
  \label{subsec:course}
  \begin{itemize}
  \item Instructor information.
  \item Course website: \href{https://web.math.rochester.edu/courses/current/201/}{https://web.math.rochester.edu/courses/current/201/}
  \item Textbook: on Blackboard.
  \item Grade: $10\%$ Webwork, $10\%$ Written Homework, $20\%$ First Midterm,
    $20\%$ Second Midterm, $40\%$ Final.
  \item Webwork: due every Friday at $11:59$ pm. Set $00$ due Friday, Jan 27;
    Set $1$ due Friday, Feb 3.
  \item Written Homework: $2$ or $3$ problems, due in class on Wednesdays. The
    first one is due Wednesdays, Feb 1. Write down your name and student ID, and
    staple it if it has more than one page.
  \item No late submissions! No makeup for exam!
  \item Old exams.
  \item Office hours: $4:30-5:30$ pm on MWF at Hylan $1008$.
  \end{itemize}
  \subsection{Objective and expectation}
  \label{subsec:objective}
  \textbf{Objective}

  Learn how to describe uncertainty in term of mathematical models and develop
  the skill of probabilistic reasoning.

  \textbf{What to expect}

  Math takes deliberate practice. You will need to do a lot of practice problems
  to do well in this course.
  
  When studying for exams, start by doing the practice exams and redoing the
  written homework and Webwork. But don't stop there. Redo the problems again
  until the process of solving them becomes automatic. This is how you'll know
  that you've mastered the material. To use your time efficiently, focus your
  practice on the things you haven't mastered.
  
  You are expected to read the textbook and review your class notes while doing homework and studying for exams.

  This is not a calculus class; we will not just be computing things according
  to simple algorithms. This class is on a higher level. Logical reasoning will
  be very important. I will show you the fundamental concepts and techniques.
  But you will often be asked to combine these in new ways in the homework and
  on exams.

  In class, I will describe the definitions and give simple instructive examples and proofs. We want to nail the basics, because that's the foundation on which you'll build.

  Probability theory is built on set theory, so that's where we'll begin....

  \subsection{Sets}
  \label{subsec:sets}
  \begin{definition}[Sets]
    A \textbf{set} is a collection of objects, usually numbers. The objects in
    the set are called \textbf{elements} of the set.
  \end{definition}

  \begin{example}
    $A = \{1, 2,3\}$.

    $1\in A$ ($1$ is an element of $A$).

    $4 \notin A$ ($4$ is not an element of $A$).
  \end{example}

  \begin{definition}
    We write

    $\N = \text{set of positive integers} = \{1, 2, 3,\cdots\}$;

    $\mathbb{Z} = \text{set of integers} = \{0, 1, -1, 2, -2,\cdots\}$;

    $\R = \text{set of real numbers}$;

    $\varnothing = \text{empty set = set of no elements} = \{\}$.
  \end{definition}

  There is another way to describe a set:
  \[
    \{x \mid x~ \text{satisfies}~ P\} = \text{set of all elements having property}~
    P.
  \]

  \begin{example}
    \[
      [0,1] = \{x \in\R\mid 0 \le x \le 1\};
    \]
    \[
      A = \{n\in \N\mid n~ \text{is a square of a positive integer}\} = \{n^2\mid n\in \N\} = \{1 ,4,
      9, 16, 25, \cdots\}.
    \]
  \end{example}

  \begin{definition}[Subsets]
    $A$ is subset of $B$ if every element of $A$ is an element of $B$, and we
    write $A \subseteq B$.
  \end{definition}

  {\color{red} insert a venn's graph here!} 

  \begin{remark}
    $A = B$ if and only if $A\subseteq B$ and $B\subseteq A$, if and only if $A$
    and $B$ have the same elements.

    The order in the set does not matter. $\{1,2,3\} = \{2, 3, 1\}$.
  \end{remark}
  \begin{example}
    $A = \{1, 2, 3\}$, $B= \{1, 2, 3, 4, 5, 6\}$, and $C = \{7\}$. Then we have
    \[
      A \subseteq B, A\nsubseteq C, B\nsubseteq A.
    \]
  \end{example}

  \begin{definition}
    [Union and Intersection]
    \[
      A \cup B = \text{union of } A~\text{and}~B = \text{set of elements that
        belong to } A~\text{or}~B = \{x\mid x\in A~\text{{\color{red} or}}~x\in B\};
    \]
    \[
      A \cap B = \text{intersection of } A~\text{and}~B = \text{set of elements that
        belong to both } A~\text{and}~B = \{x\mid x\in A~\text{{\color{red} and} }~x\in B\}.
    \]
  \end{definition}

%   \def \setA{ (0,0) circle (1cm) }
%   \def \setB{ (1.5,0) circle (1cm) }
%   \def \myrectangle{ (-2, -1.5) rectangle (3.5, 1.5) }
%   \begin{center}
%     \begin{tikzpicture}
%       \draw \myrectangle node[below left]{$\Omega$};
%       \begin{scope}
%         \clip \setA ;
%         \fill[gray] \setB ;
%       \end{scope}
%       % start of clip scope
%       % end of clip scope
%       \draw \setA node[left] {$A$};
%       \draw \setB node[right] {$B$};
%     \end{tikzpicture}

%     \begin{tikzpicture}
% \draw (-2,-1.5) rectangle (3,1.5) node[below left]{$U$}; \fill[gray] (0,0) circle (1cm);
% \fill[gray] (1,0) circle (1cm);
% \draw (0,0) circle (1cm);
% \draw (1,0) circle (1cm); \draw (-1,1) node {$A$}; \draw (3,1) node {$B$};
% \end{tikzpicture}
%   \end{center}

 {\color{red} insert venn's graphs here!} 

  \begin{example}
    $A = \{1, 2, 3\}$ and $B = \{3, 4\}$, then $A\cup B = \{1, 2, 3, 4\}$ and
    $A\cap B = \{3\}$.
  \end{example}
  
  \begin{definition}
    [Union and Intersection of Many Sets]
    Let $A_1, A_2, \cdots$ be sets.
    \begin{align*}
      \bigcup_{i=1}^n A_i & = A_1\cup A_2\cup\cdots\cup A_n\\
      & = \{x\mid x\in
        A_1~\text{or}~x\in A_2~\text{or}~\cdots~\text{or}~x\in A_n\}\\
                          & = \{x\mid x\in A_i~\text{for some}~i\in \{1, \cdots,n\}\},
    \end{align*}
    \begin{align*}
      \bigcup_{i=1}^\infty A_i & = \bigcup_{i\in \mathbb{N}} A_i = \{x\mid x\in A_i~\text{for some}~i \in \N\},
    \end{align*}
    where ``some'' means ``as least one''.
    \begin{align*}
          \bigcap_{i=1}^n A_i & =A_1\cap A_2\cap\cdots\cap A_n\\
      & = \{x\mid x\in
        A_1~\text{and}~x\in A_2~\text{and}~\cdots~\text{and}~x\in A_n\}\\
                          & = \{x\mid x\in A_i~\text{for all}~i\in \{1, \cdots,n\}\},
    \end{align*}
    \begin{align*}
      \bigcap_{i=1}^\infty A_i & = \bigcap_{i\in \mathbb{N}} A_i = \{x\mid x\in A_i~\text{for all}~i \in \N\}.
    \end{align*}
  \end{definition}

  \begin{example}
    Let $A_1 = \{1\}, A_2 = \{1,2\}, A_3 = \{1,2,3\},\cdots$. Then
    \[
      \bigcup_{i=1}^n A_i = \{1,\cdots, n\}, \quad \bigcup_{i=1}^\infty A_i =
      \{1, 2, 3,\cdots\} = \N,
    \]
    and
    \[
      \bigcap_{i=1}^\infty A_i = \{1\},\quad \bigcap_{i=5}^{10} = A_5\cap A_6
      \cap\cdots\cap A_{10} = \{1, 2, 3, 4,5\}.
    \]
  \end{example}

  \begin{definition}
    [Disjoint Sets]
    Sets $A$ and $B$ are called \textbf{disjoint} (mutually exclusive) if $A\cap
    B = \varnothing$.

    Sets $A_1, A_2,\cdots$ are called \textbf{disjoint} (mutually exclusive) if
    $A_i\cap A_j = \varnothing$ for each pair $i, j$ with $i\neq j$.
  \end{definition}

  \begin{example}
    Let $A=\{1,2,3\}, B=\{7, 22,45\}$. Then $A\cap B =\varnothing$, and thus $A$
    and $B$ are disjoint.

    $A_1 =\{1\}, A_2=\{2\}, A_3 = \{3\},\cdots$, then $A_1, A_2,\cdots$ are disjoint.
  \end{example}

  
  \begin{definition}
    [Universe, Complement and Difference]\lecture{January 23, 2017}
    Let $\Omega$ be a \textbf{universe} set, i.e., a set that contains all the
    objects of interest in a particular context. Let $A$ and $B$ be subsets of $\Omega$.
    \begin{itemize}
    \item \textbf{Complement}: $A^c = \{x\in \Omega\mid x\notin A\}$.
    
    {\color{red} insert a venns' graph here!}
    \item \textbf{Difference}: $A\setminus B =  A - B = \{x\in\Omega\mid x\in A
    ~\text{\color{red} and}~x\notin B \} = A\cap B^c$.
    
    {\color{red} venn's graph here!} 
    \end{itemize}
  \end{definition}

  \begin{example}
    Let $\Omega = \R$, $A = [0,1] = \{x\in \R: 0\le x\le 1\}$, and $B =
    \mathbb{Z}$. Then
    \[
      A^c = (-\infty, 0)\cup (1,\infty),\quad A\setminus B = (0,1).
    \]
  \end{example}

  \textbf{Pop Quiz}: 
  \begin{itemize}
  \item $\Omega^c = \varnothing$, $\varnothing^c = \Omega$.
  \item What is $\bigcup_{n\in \mathbb{Z}} (n,n+1)$?

  Answer: $\{x\in \R\mid x\notin \Z\} = \R \setminus \Z$.
  \end{itemize}

  \textbf{Algebra of Sets}:
  \begin{itemize}
  \item $(A^c)^c = A$;
  \item $A\cap A^c = \varnothing$;
  \item $A\cup \Omega = \Omega$;
  \item $A\cap \Omega = A$.
  \end{itemize}

  \textbf{De Morgan's Laws}:
  \begin{itemize}
  \item $(A\cup B)^c = A^c \cap B^c$;
  \item $\left( \bigcup_i A_i \right)^c = \bigcap_i A_i^c$;
  \item $(A\cap B)^c = A^c \cup B^c$;
  \item $\left( \bigcap_i A_i  \right)^c = \bigcup_i A_i^c$.
  \end{itemize}
  \begin{proof}
    \begin{align*}
      x\in (A\cup B)^c & \Leftrightarrow x\notin A\cup B \Leftrightarrow x\notin A~\text{and}~x\notin B\\
                       & \Leftrightarrow x\in A^c \cap B^c \quad (\text{``$\Leftrightarrow$'' means ``if and only if'' means ``is equivalent to''}).
    \end{align*}

    Rest of proof: Exercise.
  \end{proof}

  \textbf{Distributive Properties}:
  \begin{itemize}
  \item $A\cap (B\cup C) = (A\cap B) \cup (A\cap C)$;
  \item $A\cup (B\cap C) = (A\cup B)\cap (A\cup C)$.
  \end{itemize}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Random outcomes and random variables }
\label{sec:rand-outc-rand}
  \subsection{Sample spaces and probabilities}
  \label{subsec:1.1}
  \begin{definition}
    A probability model is a mathematical description of an uncertain situation.
    It has two parts:
    \begin{enumerate}
    \item The \textbf{sample space} $\Omega$, i.e., the set of all possible
      outcomes of an experiment.
      \begin{itemize}
      \item Elements of $\Omega$ are \textbf{outcomes} (also called
        \textbf{sample points}), denoted by $\omega$.
      \item Subsets of $\Omega$ are called \textbf{events}.
      \item The set of all events is denoted by $\cF$.
      \end{itemize}
    \item The \textbf{probability measure} $\bP$.
      \begin{itemize}
      \item $\bP$ is a function from $\cF$ to $\R$;
      \item To each event $A$, it assigns a number $\bP(A)$, called
        the \textbf{probability} of $A$.
      \item The probability measure $\bP$ is sometimes called a probability law or
        probability distribution.
      \end{itemize}
    \end{enumerate}
    The triple $(\Omega, \cF, \bP)$ is called a \textbf{probability space}.
  \end{definition}
  \vspace{1em}
  \textbf{Axioms of Probability}:
  \begin{enumerate}
  \item $0\le \bP(A)\le 1$ for each event $A$;
  \item $\bP(\Omega) = 1$ and $\bP(\varnothing) =0$;
  \item If $A_1, A_2, \cdots$ is a sequence of (pairwise) disjoint events, then
    \begin{equation}
      \label{eq:additivity}
      \bP\left( \bigcup_i A_i \right) = \sum_i \bP(A_i).\quad(\text{Countable additivity})
    \end{equation}
  \end{enumerate}

  \begin{corollary}
    If $A_1, A_2, \cdots, A_n$ are disjoint events, then
    \begin{equation}
      \label{eq:add-finite}
      \bP(A_1\cup \cdots \cup A_n) = \bP(A_1) + \cdots + \bP(A_n).
    \end{equation}
  \end{corollary}

  \begin{proof}
    It is a direct consequence of~\eqref{eq:additivity} by setting $A_{n+1} =
    A_{n+2} =\cdots = \varnothing$.
  \end{proof}


  \subsection{Equally likely outcomes}
  \label{subsec:1.2}
  \textbf{Fact.} Suppose the sample space has finitely many equally likely
  outcomes, then for any event $A$,
  \[
    \bP(A) = \frac{\# A}{\# \Omega} = \frac{\text{number of elements in
        $A$}}{\text{number of elements in $\Omega$}}.
  \]
  \begin{proof}
    Say $\Omega$ has $n$ equally likely outcomes, then $\bP(\{\omega\}) = 1/n$ for
    every $\omega \in \Omega$.

    For any event $A = \{a_1, \cdots, a_k\}$,
    \[
      \bP(A) = \bP\left( \bigcup_{i=1}^k \{a_i\}\right) =
      \sum_{i=1}^k\bP(\{a_i\}) = \frac{k}{n},
    \]
    where the last equality is obtained by \eqref{eq:add-finite}.
  \end{proof}

  \begin{example}
    Roll two fair six sided dice, distinguish between them in some way: a first
    one and second one. One possible outcome is $(3,5)$.
    \begin{enumerate}
      [(1)]
    \item How many possible outcomes are there?
    \item How many ways to roll doubles?
    \item What is the probability of rolling doubles?
    \end{enumerate}
  \end{example}

  \textbf{Answer.} (1). There are $6$ possible outcomes for the first die. For
  each of these, there are $6$ possible outcomes for the second die. Then the
  total is $6\cdot 6 = 36$, i.e.,
  \[
    \Omega = \{(i,j)\mid i,j = 1,2,3,4,5,6\}.
  \]

  (2). For each outcome of the first die, there is exactly one possible outcome
  for the second die, because they have to match. So there are $6\cdot 1 = 6$
  ways to roll doubles.
  

  (3). The probability of rolling doubles is
  \[
    \bP = \frac{\# ~\text{outcomes with doubles}}{\#~\text{all possible
        outcomes}} = \frac{6}{36} = \frac{1}{6}.
  \]

  \begin{remark}
    When the sample space has finitely many equally likely outcomes, we can
    compute the probabilities by \textbf{counting}.
  \end{remark}

  \textbf{Counting Principle.} \lecture{January 25, 2017} Consider a $r$-stage process. Suppose
  \begin{itemize}
  \item There are $n_1$ possible outcomes for the $1$st stage;
  \item For each possible outcome of the $1$st stage, there are $n_2$ possible
    outcomes for the $2$nd stage;
  \item For each possible outcome of the first two stages, there are $n_3$
    possible outcomes for the $3$rd stage, $\cdots$, and so on.
  \end{itemize}
  Then the total number of possible outcomes is
  \[
    n_1\cdot n_2\cdot n_3\cdots n_r.\quad(\text{multiplication principle})
  \]

  \begin{example}
    A telephone number is a $7$-digit sequence of numbers in $\{0,1,2,\cdots,
    9\}$, but the first digit cannot be zero or one. How many distinct telephone
    numbers are there?
  \end{example}

  \textbf{Answer.} This is a $7$ stage process. There are $8$ choices for the
  first stage (or first digit), and $10$ choices for each stage after. So the
  total number is
  \[
    8\cdot 10\cdot 10\cdot 10\cdot 10\cdot 10\cdot 10 = 8\cdot 10^6.
  \]

  \begin{example}
    A screen has $N$ pixels. Each pixel can be off (black) or on (white). How
    many possible images can the screen display?
  \end{example}

  \textbf{Answer.} This is an $N$ stage process. There are $2$ choices for each
  stages: pixel on or off. The total number is $2^N$.

  \textbf{Permutations.} How many ways are there to permute (means ``arrange'',
  or ``order'') $n$ distinct items?

  \textbf{Answer.} $n$ stage process. 
  \begin{itemize}
  \item $1$st stage: choose which item goes in position $1$. There are $n$ items
    to choose from.
  \item $2$nd stage: choose which item goes in position $2$. There are $n-1$
    items to choose from.
  \item $\cdots$
  \item $n$th stage: choose which item goes in position $n$. There
    is only one item to choose.
  \end{itemize}
  Thus, the total number of ways to permute $n$ distinct items is
  \[
    n(n-1)\cdots 1 = {\color{red} n!}.
  \]

  \begin{example}
    How many ways can we line up all $60$ students in a class?
  \end{example}
  \textbf{Answer.} $60\cdot 59\cdots 1 = 60! \approx 8\cdot 10^{81}$ (greater
  than the number of atoms in the observable universe).

  \textbf{Ordered Selection.} ($k$-permutations) It is a selection of items from
  a set such that the order we select them matters.

  How many ordered selections of $k$ items from a set of $n$ distinct items are
  possible?

  $k$ stage process:
  \begin{itemize}
  \item $1$st stage: choose which item goes in position one: $n$ choices.
  \item $2$nd stage: choose which item goes in position two: $n-1$ choices.
  \item $\cdots$
  \item $k$th stage: choose which item goes in position $k$: ${\color{red}
      n-k+1} $ choices.
  \end{itemize}

  So the total number is
  \[
    n\cdot n-1\cdots (n-k+1) = (n)_k \quad(\text{Pochhammer symbol or the
      descending factorial}).
  \]

  \begin{remark}
   Permutation is a special ordered selection. 
  \end{remark}

  \begin{example}
    How many ways we can line up $5$ of the $60$ students in the class?
  \end{example}

  \textbf{Answer.} $60\cdot 59\cdot 58\cdot 57\cdot 56 = 655,381,440$.

  \textbf{Combinations.} (unordered selection) It is a selection of items from a
  set such that the order of the selection does not matter.

  How many combinations of $k$ items from a set of $n$ distinct items are
  possible?

  We call this number ``$n$~choose~$k$'', denoted by
  \[
    \binom{n}{k}.
  \]

  How to find this number? We will start from the ordered selection.
  
  Given a set of $n$ distinct items. Making an ordered selection of $k$ items is
  the same as choosing a combination of $k$ items and then ordering them.

  This is a $2$ stage process.

  The number of ordered selection of $k$ items is equal to the number of
  combinations of $k$ items from $n$ distinct items times the number of ways to
  order the $k$ items:
  \[
    n(n-1)\cdots(n-k+1) = \binom{n}{k}\cdot k!.
  \]
  Therefore, the number of combinations of $k$ items from a set of $n$ distinct
  items is
  \[
    \binom{n}{k} = \frac{n(n-1)\cdots (n-k+1)}{k!} = \frac{n!}{k!(n-k)!}.
  \]
  
  \begin{example}
    Select $5$ of $60$ students in class without regard to order.
  \end{example}
  \textbf{Answer.} $\ds\binom{60}{5}$, or
  \[
    \frac{60\cdot 59\cdot 58 \cdot 57 \cdot 56}{5\cdot 4 \cdot 3\cdot 2\cdot 1}
    = 5,461,512 = \frac{60!}{5!55!}.
  \]
  \textbf{Facts.}
  \begin{itemize}
  \item $\ds\binom{n}{k} = \binom{n}{n-k}$.
  \item $\ds\binom{n}{k}$ is also called a binomial coefficient because it is equal
    to the coefficient of $x^ky^{n-k}$ in the expansion
    \[
      (x+y)^n = \sum_{k=0}^n \binom{n}{k}x^k y^{n-k}.
    \]
  \end{itemize}

  \textbf{Partitions.} A combination is a choice of a $k$-element subset of an
  $n$-element set, and the order does not matter.

  This is the same as partitioning the set into two parts. One part contains the
  $k$ elements, and the other part contains the remaining $n-k$ elements.

  Now consider partitions into more than two parts.

  Given a set of $n$ distinct items and non-negative integers $n_1, n_2,\cdots,
  n_r$ with $n=n_1+n_2+\cdots n_r$. How many ways can the set be partitioned
  into $r$ disjoint subsets with $n_i$ items in the $i$th subset? We call this
  number $\ds \binom{n}{n_1,n_2,\cdots,n_r}$.

  This is a $r$ stage process:
  \begin{itemize}
  \item Stage $1$: to form the first subset, choose a combination of $n_1$ items
    out of the $n$-item set.
  \item Stage $2$: to form the second subset, choose a combination of $n_2$
    items out the remaining $n-n_1$ items.
  \item and so on...
  \end{itemize}

  The total number of partition is
  \begin{align*}
    &\binom{n}{n_1}\binom{n-n_1}{n_2}\binom{n-n_1-n_2}{n_3}\cdots\binom{n-n_1-\cdots
    - n_{r-1}}{n_r} \\
    =&\ \frac{n!}{n_1!(n-n_1)!}\cdot \frac{(n-n_1)!}{n_2!(n-n_1-n_2)!}\cdot \frac{(n-n_1-n_2)!}{n_3!(n-n_1-n_2-n_3)!}\cdots \frac{(n-n_1-\cdots -n_{r-1})!}{n_r!(n-n_1-\cdots - n_{r-1}-n_r)!}\\
    =&\ \frac{n!}{n_1!\cdot n_2!\cdots n_r!}.
  \end{align*}

  Therefore,
  \[
    \binom{n}{n_1,n_2,\cdots, n_r} = \frac{n!}{n_1!\cdot n_2!\cdots n_r!}.
  \]

  \begin{example}
    How many arrangements are there of the letters ``BANANA''? 
  \end{example}

  This is an example of arrangement with identical objects.

  \textbf{Solution 1.} There are $6$ positions for the letters. Each arrangement
  is a partition of the set of $6$ positions into a subset of size $3$ (the
  positions that get the letter ``A''), a subset of size $2$ (the positions that
  get the letter ``N'') and a subset of size $1$ (the position that gets letter
  ``B''). For example,
  \[
    \underset{1}{\text{A}}\underset{2}{\text{A}}\underset{3}{\text{A}}\underset{4}{\text{B}}
    \underset{5}{\text{N}} \underset{6}{\text{N}}.
  \]
  
  So the total number of arrangements is
  \[
    \binom{6}{3,2,1} = \frac{6!}{3!2!1!} = 60.
  \]

  \textbf{Solution 2.} We first pretend the $6$ letters are distinct: 
  \[
    \text{B, A$_1$, N$_1$, A$_2$, N$_2$, A$_3$}.
  \]
  Then there are $6!$ ways to arrange them. But each of the $3!$ ways to arrange
  the ``A''s and each of the $2!$ ways to arrange the ``N''s correspond to the
  same arrangement. For example, 
  \[
    \text{BA$_1$N$_1$A$_2$N$_2$A$_3$ and BA$_3$N$_2$A$_1$N$_1$A$_2$}
  \]
  both spell ``BANANA''. So we need to divide it by $3!2!$ and thus, there are
  $\ds \frac{6!}{3!2!}$ ways to arrange the letters.

  \textbf{Sampling.}\lecture{January 30, 2017} It is a fundamental mechanism for producing equally likely
  outcomes. Sampling simply means drawing uniformly random elements from a given
  set:
  \begin{itemize}
  \item sampling with replacement
  \item sampling without replacement
  \item ordered sample
  \item unordered sample
  \end{itemize}
  \textbf{Exercise:} read examples in textbook.
  \begin{example}
    An urn contains $6$ red balls, $5$ blue balls. We assume the balls can be distinguished. Sample $3$ balls without replacement.
    What is the probability that one of the $3$ balls is red and the other two
    are blue.
  \end{example}

  \textbf{Solution with order.} In this case, the sample space consists of all
  possible ordered selections of $3$ balls, i.e., $11\cdot 10\cdot 9 = 990$
  outcomes. Each outcome is equally likely.

  There are $6\cdot 5\cdot 4 = 120$ outcomes, where the first ball is red and
  the other two are blue; $5\cdot 6\cdot 4 = 120$ possible outcomes, where the
  first is blue, second is red and the third is blue; $5\cdot 4\cdot 6=120$
  possible outcomes, where the first two are blue and the last is red.

  Thus, the event that one ball is red and two balls are blue consists of
  $3\cdot 120 = 360$ outcomes. Then the desired probability is $\ds
  \frac{360}{990}= \frac{4}{11}$.

  \textbf{Solution without order.} In this case, the sample space consists of
  all possible combinations of $3$ balls. There are $\ds\binom{11}{3}$ possible
  outcomes. The number of favorable outcomes is $\ds\binom{6}{1}\binom{5}{2}$,
  which is the number of ways to pick a combination of one red ball and two blue
  balls ($2$ stage process).

  Therefore, the desired probability is
  \[
    \frac{\frac{6!}{5!1!}\cdot \frac{5!}{2!3!}}{\frac{11!}{8!3!}} = \frac{4}{11}.
  \]
  
  
  \subsection{Infinitely many outcomes}
  \label{subsec:1.3}
  \begin{example}
    Flip a fair coin until the first head appears. The number of flips required
    is the outcome of the experiment. The sample space is
    \[
      \Omega = \{1, 2,\cdots\} = \N.
    \]
    What is the probability $\bP(\{k\})$ for $k\in \N$?
  \end{example}
  The outcome is $k$ if and only if the first $k-1$ flips are tails and the
  $k$th flip is heads:
  \[
    \underbrace{TT\cdots T}_{k-1}\underbrace{H}_{k\text{th}}
  \]
  There are $2^k$ possibilities for the first $k$ flips. All are equally likely
  (fair coin). Exactly one possibility that gives us what we want, so
  \[
    \bP(\{k\}) = \frac{1}{2^k}.
  \]
  This is the \textbf{geometric distribution} with success parameter $1/2$ on
  the positive integers.
  \begin{remark}
    We know that $\bP(\Omega) = 1$, so we can derive a series formula:
    \[
      \sum_{k=1}^\infty \frac{1}{2^k} = 1.
    \]
  \end{remark}
  \begin{example}
    \label{eg:uniform}
    Choose a real number uniformly at random from the interval $[0,1]$.
    ``Uniformly'' means that the chosen number is equally likely to lie
    anywhere in the interval. The sample space is
    \[
      \Omega=[0,1].
    \]
    Assume $[a,b]\subseteq [0,1]$, the probability that the chosen number lies
    in the interval $[a,b]$ should equal to the proportion $[0,1]$ covered by
    $[a,b]$:
    \[
      \bP([a,b])=b-a.
    \]
  \end{example}
    We call the probability model in Example \ref{eg:uniform} the
    \textbf{uniform distribution} on $[0,1]$.

    \textbf{Fact.} In Example \ref{eg:uniform}, we have
    \[
      \bP(\{c\}) = 0,\quad c\in [0,1].
    \]
    \begin{proof}
      We will show that $\bP(\{c\})<\e$ for any positive number $\e$.

      Given any $\e>0$. Choose an interval $[a,b]\subseteq [0,1]$ such that
      $c\in [a,b]$ and $b-a<\e$. Then
      \begin{align*}
        \bP(\{c\})
        & \le \bP(\{c\}) + \bP([a,b]\setminus \{c\}) \\
        & = \bP(\{c\} \cup ([a,b]\setminus \{c\}))\\
        & = \bP([a,b]) = b-a<\e.
      \end{align*}
      Therefore, $\bP(\{c\}) = 0$.
    \end{proof}
 
 \begin{example}
   What if we change the sample space to $[-1,1]$ in the previous example? Then
   the probability that the chosen number lies in the interval $[a,b]\subseteq
   [-1,1]$ is $(b-a)/2$.
 \end{example}
 \begin{example}
   Choose a point uniformly from a region $R$ in $\R^2$. If $A$ is a region
   contained in $R$, then the probability that the chosen point is in $A$ equals
   the proportion of $R$ covered by $A$ is:
   \[
     \bP(A) = \frac{\text{area of}~A}{\text{area of}~R}.
   \]
 \end{example}

 \textbf{Countable Sample Spaces.}
 \begin{definition}
   A set is called \textbf{countable} if it has finite elements or it can be
   written as an infinite list, i.e.,
   \[
     \{a_1,a_2,\cdots, a_n\}\quad\text{or}\quad\{a_1,a_2,\cdots\}.
   \]
 \end{definition}
 For example, $\N, \Z, \Z^2 = \{(a,b)\mid a, b\in \Z\}$, and $\mathbb{Q}=$ set
 of all rational numbers.

 \textbf{Fact.} If the sample space is countable, the probability measure $\bP$
 is completely determined by its values on single outcomes: $\bP(\{\omega\})$.
 That is, if $A=\{a_1,a_2,\cdots, a_n\}$ or $A=\{a_1,a_2,\cdots\}$ is an event,
 then $\bP(A) = \sum_i \bP(\{a_i\})$.

 \begin{definition}
   A set is called \textbf{uncountable}, if it is not countable. 
 \end{definition}
 For example, $\R$, $[0,1]$, etc.

 \textbf{Fact.} If the sample space is uncountable, the probability measure
 $\bP$ is not determined by its values on single outcomes because
 $\bP(\cup_{a\in A} \{a\})$ is not equal to $\sum_{a\in A} \bP(\{a\})$.

 
  \subsection{Consequences of probability axioms}
  \label{subsec:1.4}
  Recall: Probability Axioms
  \begin{enumerate}
  \item $0\le \bP(A)\le 1$ for each event $A$;
  \item $\bP(\Omega) = 1$ and $\bP(\varnothing) =0$;
  \item If $A_1, A_2, \cdots$ is a sequence of (pairwise) disjoint events, then $  \bP\left( \bigcup_i A_i \right) = \sum_i \bP(A_i)$.
  \end{enumerate}
  \textbf{Consequences.}
  \begin{enumerate}[(1)]
  \item $\bP(A^c) = 1- \bP(A)$. (complement)
  \item If $A\subseteq B$, then $\bP(A) \le \bP(B)$. (monotonicity)
  \item $\bP(A\cup B) \le \bP(A) + \bP(B)$. (sub-additivity)
  \item $\bP(A\cup B) = \bP(A) + \bP(B) - \bP(A\cup B)$. (inclusion-exclusion)
  \end{enumerate}
  \begin{proof}
    (1). $1 = \bP(\Omega) = \bP(A\cup A^c) = \bP(A) + \bP(A^c)$.

    (2). Since $\bP(B) = \bP(A\cup (B\setminus A)) = \bP(A) + \bP(B\setminus
    A)$, $\bP(B) \ge \bP(A)$.

    (3). It is a direct result of (4). But here we provide an alternative proof.
    Using (2),
    \[
      \bP(A\cup B) = \bP(A\cup (B\setminus A)) = \bP(A) + \bP(B\setminus A) \le
      \bP(A) + \bP(B),
    \]
    since $B\setminus A\subseteq B$. 

    (4). This time we prove it by looking at Venn's diagrams. Think of
    probability as area. Then $A \cap B$ gets counted twice, so we have to take
    it out once.
  \end{proof}
  \begin{example}
    A town with population $100,000$ has newspapers $A$ and $B$. $10\%$ of the
    proportion read paper $A$, $30\%$ of the
    proportion read paper $B$ and $8\%$ of the
    proportion read both two papers. 
  \end{example}
  \begin{enumerate}[(a)]
  \item How many people read $A$ or $B$?
    \[
      \bP(A\cup B) = \bP(A) + \bP(B) - \bP(A\cap B) = 10\% + 30\% - 8\% = 32\%.
    \]
    So there are $100 K\cdot 32\% = 32,000$ people who read paper $A$ or $B$.
  \item How many people do not read $A$ nor $B$?
    \[
      \bP(A^c\cap B^c) = \bP((A\cup B)^c) = 1 - \bP(A\cup B) = 1-32\% = 68\%.
    \]
    So there are $68,000$ people.
  \item How many people read only one paper?
    \[
      \bP((A\cup B)\set\ (A\cap B)) = \bP(A\cup B) - \bP(A\cap B) = 32\% -8\% = 24\%.
    \]
    So there are $24,000$ people.
  \end{enumerate}
  \textbf{Generalizations.}
  \begin{itemize}
  \item [(3')] If $A_1,A_2,\cdots$ is a countable sequence of events,
    $\bP(\cup_i A_i) \le \sum_i \bP(A_i)$ (countable sub-additivity).
  \item general inclusion-exclusion formula:
    \begin{align*}
      \bP(A\cup B\cup C)
      & = \bP(A) + \bP(B) + \bP(C) - \bP(A\cap B) - \bP(A\cap C) -\bP(B\cap C) + \bP(A\cap B\cap C)
    \end{align*}
    \begin{align*}
      \bP(A\cup B\cup C\cup D)
      & = \bP(A) + \bP(B) + \bP(C) + \bP(D)\\
      & \quad - \bP(A\cap B) - \bP(A\cap C) - \bP(A\cap D) - \bP(B\cap C) -\bP(B\cap D) - \bP(C\cap D)\\
      & \quad + \bP(A\cap B\cap C) + \bP(A\cap B\cap D) + \bP(A\cap C\cap D) + \bP(B\cap C\cap D)\\
      & \quad - \bP(A\cap B\cap C\cap D).
    \end{align*}
    See Page 19 of Textbook for general $n$.
  \end{itemize}
  \begin{example}
    [Birthday problem]
    What is the probability that in the class of $60$ students, two or more have
    the same birthday?
  \end{example}
  \textbf{Answer.} It is easier to first calculate the probability of the
  complement event, which is the probability that none shares a birthday.

  The sample space is all the possible lists of birthdays for the $60$ students.
  There are $(365)^{60}$ possible lists. The desired event is all possible lists
  of birthdays where no two students share a birthday. Making such a list is a
  $60$ stage process.
  \begin{itemize}
  \item Stage 1: $365$ choices for student $1$'s birthday.
  \item Stage 2: $364$ choices for student $2$'s birthday.
  \item Stage 3: $363$ choices for student $3$'s birthday.
  \item Continue...
  \end{itemize}
  In fact, this is a sampling without replacement. There are $365\cdot 364\cdots
  306$ possible lists where none shares the same birthday. Thus,
  \[
    \bP(\{\text{none shares the same birthday}\}) = \frac{365\cdot 364\cdots
      306}{365\cdot 365\cdots 365} = 0.05877\cdots.
  \]
  Therefore,
  \[
    \bP(\{\text{two or more share birthday}\}) = 1- 0.005877\cdots = 0.994122\cdots.
  \]





  
  \subsection{Random variables: a first look}
  \label{subsec:1.5}
  \begin{definition}
    Given a probability model $(\Omega, \bP, \cF)$, a \textbf{random variable} (rv) is a
   function $X$ mapping from $\Omega$ to a set of real numbers, i.e.,
   \[
     X: \Omega \to \R.
   \]
   To each outcome $\omega\in \Omega$, $X$ assigns a real number $X(\omega)$,
   which is called an experimental value or a realization of $X$.
 \end{definition}
 
  \begin{remark}
   A random variable is a function, not a variable! By convention, we use
   capitals, like $X, Y$  and $Z$ etc for random variables. 
  \end{remark}
  \textbf{Notations.}
  \begin{itemize}
  \item $\{X = c\} = \{\omega\in \Omega\mid X(\omega) = c\}$.
  \item $\{a\le X\le b\} = \{\omega\in \Omega\mid a\le X(\omega)\le b\}$.
  \item $\{X\in B\} = \{\omega\in \Omega\mid X(\omega)\in B\}$, where $B$ could
    be any subset of $\R$.
  \item $\{X = a, Y = b\} = \{X=a~\text{and}~Y=b\} = \{X=a\}\cap \{Y = b\}$.
  \end{itemize}

  \begin{remark}
    In general, the definition of a random variable requires that all sets of
    the forms like $\{X = c\}, \{a\le X\le b\}$, etc. be events. Thus, we need a
    more careful definition of $\cF$, the set of all events. See Section 1.7 for details.
  \end{remark}
  \begin{example}
    Roll two fair six sided dice. Define:
    \begin{itemize}
    \item $X_1 =$ outcome for the first die.
    \item $X_2 =$ outcome for the second die.
    \item $X = $ sum of outcomes of the two dice.
    \item $Y =$ outcome of the second die raised to $5$th power.
    \end{itemize}
  \end{example}
  If we write the sample space as
  \[
    \Omega = \{(i,j) \mid i,j\in\{1,2,3,4,5,6\}\},
  \]
  then $X_1((i,j)) = i, X_2((i,j)) = j, X((i,j)) = i+j$ and $Y((i,j)) = j^5$.

  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conditional probability and independence}
\label{sec:cond-prob-indep}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Random variables}
\label{sec:random-variables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Approximation of the Binomial distribution}
\label{sec:appr-binom-distr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transformation of random variables}
\label{sec:transf-rand-vari}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Joint distribution of random variables}
\label{sec:joint-distr-rand}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sums and symmetry}
\label{sec:sums-symmetry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Expectation and variance in the multivariate setting}
\label{sec:expect-vari-mult}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tail bounds and limit theorems}
\label{sec:tail-bounds-limit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conditional distribution}
\label{sec:cond-distr}





\end{document}